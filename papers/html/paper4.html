<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Marcos Gomez Vazquez</title>
        <link rel="icon" type="image/x-icon" href="../../assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.7.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../../css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top" style="padding-left: 0; padding-top: 20px; width: 800px; max-width: 92vw; margin: 0 auto;">
    <!-- Navigation-->
        <div class="flex-grow-1" style="text-align: center">
            <h3 class="mb-0">Exploring the Use of Software Product Lines for the Combination of Machine Learning Models</h3>
            <br>
            <p>
                Marcos Gomez-Vazquez and Jordi Cabot
                <br>
                <a href="../pdf/paper4.pdf"><b>[pdf]</b></a>
            </p>
            <p style="text-align: left">
                <b>Best Demonstrations and Tools Paper Award</b>
                <br>
                28th ACM International Systems and Software Product Line Conference (SPLC 2024)
                <br>
                Dommeldange, Luxembourg
                <br>
                <a href="https://doi.org/10.1145/3646548.3676599">https://doi.org/10.1145/3646548.3676599</a>
            </p>

            <h4 style="text-align: left">Abstract</h4>
            <p style="text-align: justify">
                The size of Large Language Models (LLMs), and Machine Learning (ML) models in general, is a key factor of their capacity and quality of their responses. But it comes with a high cost, both during the training and the model execution phase. Recently, various model merging techniques and Mixture of Experts (MoE) architectures are gaining popularity as they enable the creation of large models by combining other existing ones (the "experts" in the MoE approach). Creating these combinations remains a deep technical task with many possible configurations to consider. In this sense, this paper aims to democratize the creation of combined ML models by presenting a product line approach to the specification and training of this type of ML architectures from an initial feature model that helps users define, among other aspects, the type of models they want to combine, the combination strategy and even, for the MoE approach, the tasks that should be associated to each expert.
            </p>

            <h4 style="text-align: left">Keywords</h4>
            <p style="text-align: left">
                Software Product Line, Feature Model, Machine Learning, Large Language Model, Model Merging, Mixture of Experts
            </p>
            <h4 style="text-align: left">Cite this paper</h4>
            <pre class="code-block"><code>
@inproceedings{10.1145/3646548.3676599,
    author = {Gomez-Vazquez, Marcos and Cabot, Jordi},
    title = {Exploring the Use of Software Product Lines for the Combination of Machine Learning Models},
    year = {2024},
    isbn = {9798400705939},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3646548.3676599},
    doi = {10.1145/3646548.3676599},
    abstract = {The size of Large Language Models (LLMs), and Machine Learning (ML) models in general, is a key factor of their capacity and quality of their responses. But it comes with a high cost, both during the training and the model execution phase. Recently, various model merging techniques and Mixture of Experts (MoE) architectures are gaining popularity as they enable the creation of large models by combining other existing ones (the "experts" in the MoE approach). Creating these combinations remains a deep technical task with many possible configurations to consider. In this sense, this paper aims to democratize the creation of combined ML models by presenting a product line approach to the specification and training of this type of ML architectures from an initial feature model that helps users define, among other aspects, the type of models they want to combine, the combination strategy and even, for the MoE approach, the tasks that should be associated to each expert.},
    booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
    pages = {26â€“29},
    numpages = {4},
    keywords = {Feature Model, Large Language Model, Machine Learning, Mixture of Experts, Model Merging, Software Product Line},
    location = {Dommeldange, Luxembourg},
    series = {SPLC '24}
}
            </code></pre>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="../../js/scripts.js"></script>
    </body>
</html>
